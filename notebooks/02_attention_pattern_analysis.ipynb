{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "635HoaxHeACq"
   },
   "source": [
    " ### 02: Attention Pattern Analysis\n",
    "\n",
    "- Analyzes how GPT-Neo-125M's 144 attention heads (12 layers × 12 heads) route information\n",
    "- Identifies specialized head types: previous-token trackers, first-token anchors, focused vs. broad\n",
    "- Shows attention evolves across layers: scattered → sequential → global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLtqUq8sWsdo"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict\n",
    "import pandas as pd\n",
    "# Setting random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNPxmZqhZv9K"
   },
   "outputs": [],
   "source": [
    "# Loading the same model\n",
    "model = \"EleutherAI/gpt-neo-125M\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393,
     "referenced_widgets": [
      "e8c5246fade4466abe183aa039757d71",
      "4d362d284c7b47b4afa00e213b425cfa",
      "0a0f727a3e264cd2bdd2d97d11349dcd",
      "6954f70a6fb242a3ad971afae23a4fcc",
      "5134e32548ae49f8a750b4fd5659c7a3",
      "7b00a9063ae5442c964f1dcbb6784cf1",
      "785a94bea57a48f5bf262980734082b8",
      "1bf2f3ba147b4e21b4d9b6c53e42cbe6",
      "e66e1826fd5840279b7a4db4cb4311f1",
      "b858f3bb619c4c3c9ec4ba2519edeb58",
      "08efc0673aca49179ae47703d49c5323"
     ]
    },
    "id": "3VbDZMAAZ9tY",
    "outputId": "62d9f85a-845f-457d-d3e1-b4f892bf257a"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    ").to(device)\n",
    "config = model.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nde7rAu8aKlA"
   },
   "source": [
    "1. Generating text while tracking attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3vvZYazaBFq",
    "outputId": "469ae995-0e11-423d-da7b-b984476573f1"
   },
   "outputs": [],
   "source": [
    "# Prompt to analyze attention patterns\n",
    "prompt = \"The Eiffel Tower is located in Paris, France. It is a famous tourist site.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "input_length = inputs['input_ids'].shape[1]\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Input tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "print(f\"Number of input tokens: {input_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZSsOKk-aPgE",
    "outputId": "461dfadf-2b5d-48e3-d7a8-b28228713c43"
   },
   "outputs": [],
   "source": [
    "# Generating with attention tracking\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=10.0,\n",
    "    return_dict_in_generate=True,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "generated_text = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(output.sequences[0])\n",
    "print(f\"\\nGenerated text:\\n{generated_text}\")\n",
    "print(f\"\\nAll tokens: {all_tokens}\")\n",
    "print(f\"Total tokens: {len(all_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OrXXUGjeqOH"
   },
   "source": [
    "- As the repetition penalty is set the model produces diverse text without getting stuck in loops.\n",
    "- The content is factually incorrect (the Eiffel Tower was built by Gustave Eiffel in 1889, not Louis-Philippe de Beaumont in 1871), showing that while the model can generate fluent text, it doesn't reliably produce accurate information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_6_4HMvaaK4"
   },
   "source": [
    "2. Visualizing attention patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIB3wbZNaX0P"
   },
   "outputs": [],
   "source": [
    "def plot_attention_heatmap(\n",
    "    attention_weights: torch.Tensor,\n",
    "    tokens: List[str],\n",
    "    layer: int,\n",
    "    head: int,\n",
    "    title: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Plotting attention heatmap for a specific layer and head.\n",
    "    Args:\n",
    "        attention_weights: Attention tensor of shape (batch, heads, seq_len, seq_len)\n",
    "        tokens: List of token strings\n",
    "        layer: Layer index\n",
    "        head: Head index\n",
    "        title: Custom title for the plot\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Extracting attention for specific head\n",
    "    attn = attention_weights[0, head].cpu().numpy()\n",
    "    # Plotting attention\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        attn,\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        cmap='viridis',\n",
    "        cbar_kws={'label': 'Attention Weight'},\n",
    "        vmin=0,\n",
    "        vmax=1\n",
    "    )\n",
    "    plot_title = title or f\"Layer {layer}, Head {head}\"\n",
    "    plt.title(plot_title, fontsize=14, pad=20)\n",
    "    plt.xlabel('Key Tokens', fontsize=12)\n",
    "    plt.ylabel('Query Tokens', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting attention from first generation step for analysis\n",
    "first_step_attentions = output.attentions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyPpihrQbDCJ"
   },
   "source": [
    "3. Identifying attention head types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V19qR70Ra1Aa"
   },
   "outputs": [],
   "source": [
    "def classify_attention_head(\n",
    "    attention_weights: torch.Tensor,\n",
    "    threshold: float = 0.5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Classifying attention head based on its patterns\n",
    "    Args:\n",
    "        attention_weights: Attention tensor of shape (seq_len, seq_len)\n",
    "        threshold: Threshold for determining strong attention.\n",
    "    Returns:\n",
    "        Dictionary with scores for different head types\n",
    "    \"\"\"\n",
    "    attn = attention_weights.cpu().numpy()\n",
    "    seq_len = attn.shape[0]\n",
    "    # Tracking previous token score: average attention to position i-1 from position i\n",
    "    prev_token_score = 0\n",
    "    if seq_len > 1:\n",
    "        prev_diag = np.array([attn[i, i-1] for i in range(1, seq_len)])\n",
    "        prev_token_score = np.mean(prev_diag)\n",
    "    # First token score: average attention to first token\n",
    "    first_token_score = np.mean(attn[:, 0])\n",
    "    eps = 1e-10\n",
    "    entropy = -np.sum(attn * np.log(attn + eps), axis=1)\n",
    "    avg_entropy = np.mean(entropy)\n",
    "    max_entropy = np.log(seq_len)\n",
    "    normalized_entropy = avg_entropy / max_entropy if max_entropy > 0 else 0\n",
    "    attention_dict={\n",
    "        'prev_token_score': prev_token_score,\n",
    "        'first_token_score': first_token_score,\n",
    "        'entropy': avg_entropy,\n",
    "        'normalized_entropy': normalized_entropy,\n",
    "        'is_focused': normalized_entropy < 0.3,\n",
    "        'is_broad': normalized_entropy > 0.7,\n",
    "    }\n",
    "    return attention_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9Qm0FXLbg0U"
   },
   "outputs": [],
   "source": [
    "head_classifications = []\n",
    "for layer_idx in range(config.num_layers):\n",
    "    layer_attns = first_step_attentions[layer_idx][0]\n",
    "    for head_idx in range(config.num_heads):\n",
    "        head_attn = layer_attns[head_idx]\n",
    "        classification = classify_attention_head(head_attn)\n",
    "        classification['layer'] = layer_idx\n",
    "        classification['head'] = head_idx\n",
    "        head_classifications.append(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isZUOpq0bmkz"
   },
   "outputs": [],
   "source": [
    "df_heads=pd.DataFrame(head_classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "NybeTY1LbtaG",
    "outputId": "824bf5cb-646b-4f71-de86-cbef29db6557"
   },
   "outputs": [],
   "source": [
    "#Previous token heads:How much each position attends to the token immediately before it (diagonal pattern)\n",
    "prev_token_heads = df_heads.nlargest(5, 'prev_token_score')[['layer', 'head', 'prev_token_score']]\n",
    "print(\"Previous token heads\")\n",
    "prev_token_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZ-kvhUih1Co"
   },
   "source": [
    "This table identifies attention heads that specialize in looking at previous tokens.\n",
    "- Layer 5 Head 8 is almost perfect with 99.96% retention.\n",
    "- Early layers like Layer 2 Head 0 show weaker specialization (48%).\n",
    "- Layer 7 shows moderate focus  (53%) indicating that the model builds local sequential dependencies strongly in the middle of network before collating the information more broadly in final layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "eqKkdYp9b6i5",
    "outputId": "630aa940-2c3c-4afd-a9e1-4d81a4b879c3"
   },
   "outputs": [],
   "source": [
    "#First token heads:How much all positions attend to the first token (vertical column pattern)\n",
    "first_token_heads = df_heads.nlargest(5, 'first_token_score')[['layer', 'head', 'first_token_score']]\n",
    "print(\"First token heads\")\n",
    "first_token_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQYEyZKrjGwh"
   },
   "source": [
    "- Layer 8 dominates first-token attention it has 4 out of 5 top first-token heads (Heads 8, 11, 0, and 1 all with ~97-98% attention to the first token).\n",
    "- this suggests that later layers use the first token as a \"summary anchor\" or aggregation point to collect global sentence-level information, which contrasts with middle layers that focus on local sequential dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "bs86vlyscOLZ",
    "outputId": "fc6e7b5e-417c-4266-bcdd-9a3ddbdfea8c"
   },
   "outputs": [],
   "source": [
    "#Most focused token heads:Attention spread (0 = focused, 1 = uniform across all tokens)\n",
    "focused_heads = df_heads.nsmallest(5, 'normalized_entropy')[['layer', 'head', 'normalized_entropy']]\n",
    "print(\"Most focused token heads\")\n",
    "focused_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MunGnlycjYM-"
   },
   "source": [
    " - Layer 5 Head 8 (entropy 0.0009) is extremely focused this is the same head we saw as the top previous-token head\n",
    " - Focused heads cluster in early-to-middle layers (0-6), laser-focusing on 1-2 specific tokens for precise pattern detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "C18MYJwZcW13",
    "outputId": "38eb4cfb-a828-45f5-e374-65f7d1a879fd"
   },
   "outputs": [],
   "source": [
    "#Most broad token heads\n",
    "broad_heads = df_heads.nlargest(5, 'normalized_entropy')[['layer', 'head', 'normalized_entropy']]\n",
    "print(\"Most broad token heads\")\n",
    "broad_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4mlZ_C5jluC"
   },
   "source": [
    "- Layer 1 dominates with 3 of the 5 broadest heads (entropy ~0.61-0.67), spreading attention widely across many tokens.\n",
    "- This shows early layers gather diverse contextual information broadly, middle layers specialize and focus sharply, and later layers (like Layer 8) use focused first-token attention for global aggregation revealing a clear computational pipeline through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OhN4JMVdcfJE",
    "outputId": "5cd83051-1a31-4405-8891-f46183e7736b"
   },
   "outputs": [],
   "source": [
    "print(\"Visualizing Previous Token Head:\")\n",
    "top_prev = prev_token_heads.iloc[0]\n",
    "plot_attention_heatmap(\n",
    "    first_step_attentions[int(top_prev['layer'])],\n",
    "    all_tokens[:input_length],\n",
    "    int(top_prev['layer']),\n",
    "    int(top_prev['head']),\n",
    "    f\"Previous Token Head - L{int(top_prev['layer'])}H{int(top_prev['head'])} (score: {top_prev['prev_token_score']:.3f})\"\n",
    ")\n",
    "print(\"\\nVisualizing First Token Head:\")\n",
    "top_first = first_token_heads.iloc[0]\n",
    "plot_attention_heatmap(\n",
    "    first_step_attentions[int(top_first['layer'])],\n",
    "    all_tokens[:input_length],\n",
    "    int(top_first['layer']),\n",
    "    int(top_first['head']),\n",
    "    f\"First Token Head - L{int(top_first['layer'])}H{int(top_first['head'])} (score: {top_first['first_token_score']:.3f})\"\n",
    ")\n",
    "print(\"\\nVisualizing Most Focused Head:\")\n",
    "top_focused = focused_heads.iloc[0]\n",
    "plot_attention_heatmap(\n",
    "    first_step_attentions[int(top_focused['layer'])],\n",
    "    all_tokens[:input_length],\n",
    "    int(top_focused['layer']),\n",
    "    int(top_focused['head']),\n",
    "    f\"Focused Head - L{int(top_focused['layer'])}H{int(top_focused['head'])} (entropy: {top_focused['normalized_entropy']:.3f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnffhC3Gj3rJ"
   },
   "source": [
    "- Previous Token Head (L5H8):\n",
    "It has perfect diagonal pattern indicating each token attends exclusively to its immediate predecessor with neat 100% focus as a pure sequential dependency tracker.\n",
    "- First Token Head(L8H8):\n",
    "Every query token highlights the first column (\"The\") showing attention focused on start as an anchor point.\n",
    "- Focused Head(L5H8): Its extreme focus comes from putting all attention on exactly one token (predecessor) making it most distinct head in network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psahBMS1cqQp"
   },
   "source": [
    "4. Attention Patterns Across Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "e5Ccnoz-cits",
    "outputId": "1866ae3a-4e5e-49d4-e5da-7c672df012b5"
   },
   "outputs": [],
   "source": [
    "# Visualizing how attention patterns change across layers\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "fig.suptitle('Attention Patterns Across All Layers (Head 0)', fontsize=16, y=1.00)\n",
    "for layer_idx in range(config.num_layers):\n",
    "    row = layer_idx // 4\n",
    "    col = layer_idx % 4\n",
    "    attn = first_step_attentions[layer_idx][0, 0].cpu().numpy()\n",
    "    im = axes[row, col].imshow(attn, cmap='viridis', vmin=0, vmax=1)\n",
    "    axes[row, col].set_title(f'Layer {layer_idx}', fontsize=10)\n",
    "    axes[row, col].set_xlabel('Keys', fontsize=8)\n",
    "    axes[row, col].set_ylabel('Queries', fontsize=8)\n",
    "    axes[row, col].tick_params(labelsize=6)\n",
    "fig.colorbar(im, ax=axes, label='Attention Weight', fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OfoT0JHlVT8"
   },
   "source": [
    "Layer 0 is scattered then Layers 2-5 develop strong diagonal pattern (previous token), layer 8 shows dominant first column attention and later layers become more distributed.\n",
    "\n",
    "**Revealing progession from local->sequential->global attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vL_w014RdRXK"
   },
   "source": [
    "5. Average attention by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoJeuMdMdY3E"
   },
   "outputs": [],
   "source": [
    "# Computing average attention statistics per layer\n",
    "layer_stats = []\n",
    "for layer_idx in range(config.num_layers):\n",
    "    layer_attns = first_step_attentions[layer_idx][0]\n",
    "    # Average across heads\n",
    "    avg_attn = layer_attns.mean(dim=0).cpu().numpy()\n",
    "    prev_token_attn = np.mean([avg_attn[i, i-1] for i in range(1, avg_attn.shape[0])])\n",
    "    first_token_attn = np.mean(avg_attn[:, 0])\n",
    "    # Computing entropy\n",
    "    eps = 1e-10\n",
    "    entropy = -np.sum(avg_attn * np.log(avg_attn + eps), axis=1)\n",
    "    avg_entropy = np.mean(entropy)\n",
    "    max_entropy = np.log(avg_attn.shape[1])\n",
    "    normalized_entropy = avg_entropy / max_entropy\n",
    "    layer_stats.append({\n",
    "        'layer': layer_idx,\n",
    "        'prev_token_attn': prev_token_attn,\n",
    "        'first_token_attn': first_token_attn,\n",
    "        'normalized_entropy': normalized_entropy,\n",
    "    })\n",
    "\n",
    "df_layers = pd.DataFrame(layer_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150
    },
    "id": "emh7ha5Adbpd",
    "outputId": "0136fcf5-dc63-47f9-df99-85560203ddca"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "# Previous token attention\n",
    "axes[0].plot(df_layers['layer'], df_layers['prev_token_attn'], marker='o', linewidth=2)\n",
    "axes[0].set_xlabel('Layer', fontsize=12)\n",
    "axes[0].set_ylabel('Avg Attention to Previous Token', fontsize=12)\n",
    "axes[0].set_title('Previous Token Attention by Layer', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "# First token attention\n",
    "axes[1].plot(df_layers['layer'], df_layers['first_token_attn'], marker='o', linewidth=2, color='orange')\n",
    "axes[1].set_xlabel('Layer', fontsize=12)\n",
    "axes[1].set_ylabel('Avg Attention to First Token', fontsize=12)\n",
    "axes[1].set_title('First Token Attention by Layer', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "# Entropy\n",
    "axes[2].plot(df_layers['layer'], df_layers['normalized_entropy'], marker='o', linewidth=2, color='green')\n",
    "axes[2].set_xlabel('Layer', fontsize=12)\n",
    "axes[2].set_ylabel('Normalized Entropy', fontsize=12)\n",
    "axes[2].set_title('Attention Entropy by Layer', fontsize=14)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQgdYJbjl9UO"
   },
   "source": [
    "Previous token attention peaks at Layer 5 (0.35), first-token attention dominates in Layers 6-10 (peaks at 0.84 in Layer 10), and entropy drops dramatically in Layer 11 (0.25) showing focused attention together revealing the network's computational strategy: **build local patterns in middle layers, aggregate globally in later layers, then focus sharply for final prediction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSXdxn9SdFal",
    "outputId": "8ee30fdf-78cc-41e7-d591-137b1f6f1c89"
   },
   "outputs": [],
   "source": [
    "print(\"\\n1. HEAD SPECIALIZATION:\")\n",
    "print(f\"   - {len(df_heads[df_heads['prev_token_score'] > 0.7])} heads show strong previous token attention (>0.7)\")\n",
    "print(f\"   - {len(df_heads[df_heads['first_token_score'] > 0.5])} heads show strong first token attention (>0.5)\")\n",
    "print(f\"   - {len(df_heads[df_heads['is_focused']])} heads are highly focused (entropy < 0.3)\")\n",
    "print(f\"   - {len(df_heads[df_heads['is_broad']])} heads show broad attention (entropy > 0.7)\")\n",
    "print(\"\\n2. LAYER-WISE PATTERNS:\")\n",
    "early_layers = df_layers[df_layers['layer'] < 4]\n",
    "late_layers = df_layers[df_layers['layer'] >= 8]\n",
    "print(f\"   - Early layers (0-3) avg entropy: {early_layers['normalized_entropy'].mean():.3f}\")\n",
    "print(f\"   - Late layers (8-11) avg entropy: {late_layers['normalized_entropy'].mean():.3f}\")\n",
    "print(f\"   - Early layers prev token attn: {early_layers['prev_token_attn'].mean():.3f}\")\n",
    "print(f\"   - Late layers prev token attn: {late_layers['prev_token_attn'].mean():.3f}\")\n",
    "print(\"\\n3. ATTENTION DIVERSITY:\")\n",
    "print(f\"   - Entropy range: {df_heads['normalized_entropy'].min():.3f} to {df_heads['normalized_entropy'].max():.3f}\")\n",
    "print(f\"   - Average entropy across all heads: {df_heads['normalized_entropy'].mean():.3f}\")\n",
    "print(f\"   - Std dev of entropy: {df_heads['normalized_entropy'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "GPT-Neo-125M exhibits strong attention specialization, with early layers exploring broad context and later layers consolidating information through focused, low-entropy heads rather than sequential token tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hU8CHilOmMx9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
